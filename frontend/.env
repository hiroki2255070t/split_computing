# モデル定義（model.json）のファイルパス
# VITE_MODEL_FILE_PATH = /model/efficientnet_b0/model.json 
# VITE_MODEL_FILE_PATH = /model/efficientnetV2_S/model.json
VITE_MODEL_FILE_PATH = /model/onnx/efficientNetV2_S_head_3.onnx

# 使用する機械学習ライブラリ（tfjs or onnx）
# VITE_BACKEND_LIBRARY = tfjs
VITE_BACKEND_LIBRARY = onnx

# 入力層の名前(tfjsの場合"input_0", onnxの場合"image_tensor")
VITE_INPUT_LAYER_NAME = image_tensor

# 分割点に該当する層の名前（model.jsonから取得）
VITE_SPLIT_LAYER_NAME = StatefulPartitionedCall/efficientnetv2-s/blocks_25/tpu_batch_normalization_2/FusedBatchNormV3
# VITE_SPLIT_LAYER_NAME = StatefulPartitionedCall/efficientnetv2-b0/blocks_13/depthwise_conv2d/depthwise

# モデルの学習時の入力画像サイズ
VITE_INPUT_IMAGE_SIZE_EFFICIENT_NET_V2_B0 = 224
VITE_INPUT_IMAGE_SIZE_EFFICIENT_NET_V2_S = 384

VITE_WEBSOCKET_URL = ws://localhost:8080 # WebSockerサーバのURL
# VITE_WEBSOCKET_URL = wss://ae4ea6830476.ngrok-free.app # WebSockerサーバのURL（ngrokを用いてローカルPCで起動）
VITE_RECONNECT_DELAY_MS = 1000 # WebSocketの再接続を試みる間隔
VITE_MAX_RECONNECT_ATTEMPTS = 10 # WebSocketの再接続を試みる回数
VITE_FRAME_CAPTURE_INTERVAL_MS = 100 # ビデオからフレームをキャプチャする間隔（現在は使用していない）
